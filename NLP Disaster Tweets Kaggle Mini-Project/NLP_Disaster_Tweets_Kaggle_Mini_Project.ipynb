{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div align=\"center\" style=\"border: 2px solid #1C6EA4; background-color: #90EE90; padding: 15px;\">\n    <img src=\"https://www.colorado.edu/brand/sites/default/files/styles/medium/public/page/boulder_left_lockup_black.png?itok=4qMuKoBT\" alt=\"Colorado Boulder University Logo\" width=\"400\" height=\"500\">\n    <h2 style=\"color: black; font-weight: bold;\">\n        <i class=\"fas fa-exclamation-triangle\" style=\"color: #FF4500;\"></i> NLP Disaster Tweets Kaggle Mini-Project\n    </h2>\n</div>\n\n\n**Name**: _Willian Pina_\n\n<div style=\"background-color: lightgrey; color: black; padding: 20px;border: 1px solid black;\">\n  <h3>Brief Description of the Problem</h3>\n  <p>The challenge is to develop a machine learning model capable of classifying tweets as either related to a real-world disaster or not. This is a Natural Language Processing (NLP) problem where the objective is to understand the semantics and context of textual dataâ€”in this case, tweets. The aim is to automate a task that is easily done by humans but not so straightforward for machines: discerning the urgency or critical nature of a tweet based on its text content.</p>\n\n  <h3>NLP (Natural Language Processing)</h3>\n  <p>NLP is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate objective is to enable computers to understand, interpret, and produce human languages in a way that is both meaningful and useful. NLP involves several challenges including language modeling, parsing, sentiment analysis, and machine translation.</p>\n\n  <h3>Data Description</h3>\n  <h4>Size:</h4>\n  <ul>\n    <li>The training set comprises 7,613 manually labeled tweets.</li>\n    <li>The test set contains 3,263 tweets.</li>\n  </ul>\n\n  <h4>Dimension:</h4>\n  <p>Each tweet is represented by several features, making the data multidimensional. Specifically, the features are:</p>\n  <ul>\n    <li><code>id</code>: A unique identifier for each tweet.</li>\n    <li><code>text</code>: The text of the tweet itself.</li>\n    <li><code>location</code>: The geographical location from which the tweet was sent (may be blank).</li>\n    <li><code>keyword</code>: A keyword extracted from the tweet (may also be blank).</li>\n    <li><code>target</code>: A label indicating whether the tweet is about a real disaster (1) or not (0).</li>\n  </ul>\n\n  <h4>Structure:</h4>\n  <p>The data is structured in a tabular format, stored in CSV files. There are separate files for training (<code>train.csv</code>) and testing (<code>test.csv</code>). A sample submission file (<code>sample_submission.csv</code>) is also provided to guide how predictions should be formatted for submission.</p>\n\n  <p>By understanding these aspects of the data and problem, we can proceed to design and train a machine learning model to make accurate predictions.</p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom tqdm import tqdm\nimport random\n\n# Importing necessary libraries for data preprocessing and model building\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout,GRU, Bidirectional, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\nimport keras_tuner\nfrom kerastuner.tuners import RandomSearch\n\nnp.random.seed(42)\nrandom.seed(42)\ntf.random.set_seed(42)\n\n\nclass CONFIG:\n    train_path             = '/kaggle/input/nlp-getting-started/train.csv'\n    test_path              = '/kaggle/input/nlp-getting-started/test.csv'\n    sample_submission_path = '/kaggle/input/nlp-getting-started/sample_submission.csv'\n    max_vocab_size         = 20000  # Maximum number of words to keep, based on word frequency\n    max_sequence_length    = 100  # Maximum number of words in a sequence\n    embedding_dim          = 100  # Dimensionality of the GloVe word vectors\n    epochs                 = 5\n    batch_size             = 32","metadata":{"execution":{"iopub.status.busy":"2023-10-25T11:01:57.312661Z","iopub.execute_input":"2023-10-25T11:01:57.313096Z","iopub.status.idle":"2023-10-25T11:01:57.349092Z","shell.execute_reply.started":"2023-10-25T11:01:57.313035Z","shell.execute_reply":"2023-10-25T11:01:57.348136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load Data\ntrain_data = pd.read_csv(CONFIG.train_path)\ntest_data = pd.read_csv(CONFIG.test_path)\n\n# Describe Size and Dimension\nprint(f\"Train Data Dimensions: {train_data.shape}\")\nprint(f\"Test Data Dimensions: {test_data.shape}\")\n\n# Check Structure\nprint(\"\\nTrain Data Structure:\")\nprint(train_data.info())\nprint(\"\\nTest Data Structure:\")\nprint(test_data.info())","metadata":{"execution":{"iopub.status.busy":"2023-10-25T11:01:57.352337Z","iopub.execute_input":"2023-10-25T11:01:57.353140Z","iopub.status.idle":"2023-10-25T11:01:57.425206Z","shell.execute_reply.started":"2023-10-25T11:01:57.353093Z","shell.execute_reply":"2023-10-25T11:01:57.423891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: lightgrey; color: black; padding: 20px; border: 1px solid black;\">\n  <h4>Exploratory Data Analysis (EDA)</h4>\n  \n  <p>EDA is crucial for understanding the dataset's characteristics, identifying patterns and potential outliers, and informing subsequent data processing steps. Below are some insights gained from EDA, complete with visualizations and data cleaning procedures.</p>\n  \n  <h5>Data Visualizations</h5>\n  \n  <ul>\n    <li><strong>Target Variable Distribution</strong>: A histogram of the <code>target</code> variable can show us how balanced or imbalanced our dataset is.</li>\n    <li><strong>Missing Values</strong>: Bar plots can show the number of missing values in each feature.</li>\n    <li><strong>Word Count in Tweets</strong>: A histogram can provide an understanding of the text length in the tweets.</li>\n    <li><strong>Most Common Keywords</strong>: A bar plot can display the most frequently occurring keywords in the dataset.</li>\n  </ul>\n</div>\n\n\n","metadata":{}},{"cell_type":"code","source":"# Plot Histogram for 'target' distribution\nsns.countplot(x='target', data=train_data)\nplt.title('Distribution of Target Variable')\nplt.show()\n\n# Check for Missing Values\nprint(\"\\nMissing Values in Train Data:\")\nprint(train_data.isnull().sum())\n\nprint(\"\\nMissing Values in Test Data:\")\nprint(test_data.isnull().sum())\n\n# Word Count Distribution\nword_counts = train_data['text'].apply(lambda x: len(x.split()))\nsns.histplot(word_counts, bins=30)\nplt.title('Word Count Distribution in Tweets')\nplt.show()\n\n# Data Cleaning (Simple)\ntrain_data.fillna('', inplace=True)\ntest_data.fillna('', inplace=True)\n\n# Most Common Keywords\ncommon_keywords = Counter(train_data['keyword'])\ncommon_keywords = common_keywords.most_common(10)\nsns.barplot(x=[count for keyword, count in common_keywords], y=[keyword for keyword, count in common_keywords])\nplt.title('Most Common Keywords')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-25T11:01:57.427032Z","iopub.execute_input":"2023-10-25T11:01:57.430613Z","iopub.status.idle":"2023-10-25T11:01:58.393312Z","shell.execute_reply.started":"2023-10-25T11:01:57.430565Z","shell.execute_reply":"2023-10-25T11:01:58.392110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: lightgrey; color: black; padding: 20px; border: 1px solid black;\">\n  <h4>Data Cleaning Procedures</h4>\n  \n  <ul>\n    <li><strong>Handling Missing Values</strong>: Fill missing <code>keyword</code> and <code>location</code> values with a placeholder like \"None\".</li>\n    <li><strong>Text Cleaning</strong>: Remove URLs, special characters, and perform lowercase conversion for text normalization.</li>\n  </ul>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Handling Missing Values\ntrain_data.fillna('None', inplace=True)\ntest_data.fillna('None', inplace=True)\n\n# Text Cleaning\ntrain_data['text'] = train_data['text'].str.replace('http\\S+|www.\\S+', '', case=False)\ntest_data['text'] = test_data['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n\nprint(\"Train Data\")\ndisplay(train_data.head())\nprint(\"\\nTest Data\")\ndisplay(test_data.head())","metadata":{"execution":{"iopub.status.busy":"2023-10-25T11:01:58.396493Z","iopub.execute_input":"2023-10-25T11:01:58.396969Z","iopub.status.idle":"2023-10-25T11:01:58.456632Z","shell.execute_reply.started":"2023-10-25T11:01:58.396915Z","shell.execute_reply":"2023-10-25T11:01:58.455709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: lightgrey; color: black; padding: 20px; border: 1px solid black;\">\n  <h4>Plan of Analysis</h4>\n  \n  <p>Based on the EDA:</p>\n  \n  <ul>\n    <li><strong>Data Imbalance</strong>: The target variable is somewhat imbalanced, with more tweets being not about real disasters. Techniques like SMOTE or random undersampling could be considered.</li>\n    <li><strong>Feature Engineering</strong>: Given the missing values in <code>keyword</code> and <code>location</code>, these columns could either be filled with a placeholder or used for feature engineering. Tokenization and padding will be essential for the <code>text</code> column.</li>\n    <li><strong>Model Selection</strong>: The text nature of the data confirms that NLP models like LSTM or BERT would be beneficial for this problem.</li>\n    <li><strong>Evaluation</strong>: F1-Score remains the metric of choice, aligning with the competition's requirements.</li>\n  </ul>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: lightgrey; color: black; padding: 20px; border: 1px solid black;\">\n  <h3>Model Architecture</h3>\n  \n  <h4>Architecture Overview</h4>\n  <p>The proposed architecture for this task is centered around Long Short-Term Memory (LSTM) layers. The model consists of the following components:</p>\n  <ul>\n    <li><strong>Embedding Layer</strong>: Utilized for converting tokenized words into vectors.</li>\n    <li><strong>LSTM Layer</strong>: Employed to capture sequential dependencies in the text data.</li>\n    <li><strong>Dropout Layer</strong>: Integrated for regularization purposes to mitigate overfitting.</li>\n    <li><strong>Dense Layer</strong>: A fully connected layer designed for classification.</li>\n    <li><strong>Output Layer</strong>: Equipped with a sigmoid activation function for binary classification.</li>\n  </ul>\n  \n  <h4>Rationale</h4>\n  <ul>\n    <li><strong>Embedding Layer</strong>: Word embeddings are instrumental in capturing semantic relationships between words by providing a dense vector space representation.</li>\n    <li><strong>LSTM Layer</strong>: LSTMs are adept at learning from long sequences and are not prone to the vanishing gradient problem.</li>\n    <li><strong>Dropout Layer</strong>: This layer aids in model generalization by reducing the risk of overfitting.</li>\n    <li><strong>Dense and Output Layer</strong>: These layers are standard in machine learning pipelines for classification tasks.</li>\n  </ul>\n  \n  <h4>Word Embedding Strategy: GloVe (Global Vectors for Word Representation)</h4>\n  <p>GloVe is recommended for word embedding due to its ability to capture both global and local semantic relationships between words.</p>\n  \n  <h5>Working Mechanism of GloVe</h5>\n  <ol>\n    <li>A co-occurrence matrix is constructed from the dataset, wherein the elements \\( X_{ij} \\) denote the frequency of word \\( i \\) appearing adjacent to word \\( j \\).</li>\n    <li>Matrix factorization techniques are then applied to this matrix to obtain dense word vectors capable of capturing a variety of word relationships.</li>\n  </ol>\n  \n  <h4>Building and Training Approach</h4>\n  <ul>\n    <li><strong>Data Preprocessing</strong>: Tokenization of the textual data followed by sequence padding.</li>\n    <li><strong>Word Embedding</strong>: Adoption of pre-trained GloVe embeddings.</li>\n    <li><strong>Model Construction</strong>: Implementation of the LSTM-based architecture as outlined above.</li>\n    <li><strong>Model Compilation</strong>: Utilization of binary cross-entropy as the loss function and the Adam optimizer.</li>\n    <li><strong>Model Training</strong>: Execution of model training on the dataset, designating a subset for validation.</li>\n    <li><strong>Performance Evaluation</strong>: Application of the F1-Score metric, aligning with competition guidelines.</li>\n  </ul>\n  \n  <h4>References</h4>\n  <ul>\n    <li>Pennington, J., Socher, R., & Manning, C. (2014). GloVe: Global Vectors for Word Representation.</li>\n    <li>Hochreiter, S., & Schmidhuber, J. (1997). Long Short-Term Memory.</li>\n  </ul>\n  \n  <p>Through the incorporation of GloVe embeddings and LSTM layers, the model aims to encapsulate both the semantic and sequential nuances inherent in the textual data, thereby enhancing its capacity for accurate tweet classification in the given problem context.</p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Data Preprocessing\n# Tokenization\ntokenizer = Tokenizer(num_words=CONFIG.max_vocab_size)\ntokenizer.fit_on_texts(train_data['text'])\nsequences = tokenizer.texts_to_sequences(train_data['text'])\n\n# Padding sequences\ndata = pad_sequences(sequences, maxlen=CONFIG.max_sequence_length)\n\n# Prepare labels\nlabels = np.array(train_data['target'])\n\n# Splitting the data into training and validation sets\nprint(\"Splitting the data into trainind and validation set...\")\nX_train, X_val, y_train, y_val = train_test_split(data, labels, test_size=0.2, random_state=42)\n\n# Model Building\nprint(\"\\nModel building...\")\nmodel = Sequential()\nmodel.add(Embedding(CONFIG.max_vocab_size, CONFIG.embedding_dim, input_length=CONFIG.max_sequence_length))\nmodel.add(LSTM(50, return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(50))\nmodel.add(Dense(1, activation='sigmoid'))\nprint(\"\\n---------> OK\")\n\n# Model Compilation\nprint(\"\\nModel compilation...\")\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nprint(\"\\n---------> OK\")\n\n# Model Training\nprint(\"\\nModel training...\")\nprint(\"====================\\n\")\nmodel.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=CONFIG.epochs, batch_size=CONFIG.batch_size)\nprint(\"\\n---------> OK\")\n\n# Model Evaluation\nprint(\"\\nModel evaluation...\")\ny_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\nf1 = f1_score(y_val, y_pred)\nprint(\"\\n---------> OK\")\n\nprint(\"\\n\\nF1 Score:\", f1)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T11:01:58.458203Z","iopub.execute_input":"2023-10-25T11:01:58.458523Z","iopub.status.idle":"2023-10-25T11:04:03.318795Z","shell.execute_reply.started":"2023-10-25T11:01:58.458497Z","shell.execute_reply":"2023-10-25T11:04:03.317784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: lightgrey; color: black; padding: 20px; border: 1px solid black;\">\n  <h4>Hyperparameter Tuning</h4>\n  \n  <ul>\n    <li><strong>Batch Size</strong>: Experiment with different batch sizes (32, 64, 128, etc.) to see how they affect training speed and model performance.</li>\n    <li><strong>Learning Rate</strong>: Try varying learning rates with the Adam optimizer to observe its impact on model convergence.</li>\n    <li><strong>LSTM Units</strong>: Experiment with the number of LSTM units to see if more complex representations help.</li>\n  </ul>\n  \n  <h4>Different Architectures</h4>\n  \n  <ul>\n    <li><strong>GRU vs LSTM</strong>: Compare the performance of Gated Recurrent Units (GRU) with LSTM to see if one outperforms the other for this task.</li>\n    <li><strong>Bidirectional LSTM</strong>: Using bidirectional LSTMs can help the model capture context from both the past and the future tokens in the sequence.</li>\n    <li><strong>Stacked LSTM</strong>: Experiment with stacking multiple LSTM layers on top of each other.</li>\n  </ul>\n  \n  <h4>Techniques to Improve Training or Performance</h4>\n  \n  <ul>\n    <li><strong>Dropout Rate</strong>: Try different dropout rates to see the model's robustness and its ability to generalize.</li>\n    <li><strong>Batch Normalization</strong>: Incorporate batch normalization layers to stabilize and perhaps accelerate training.</li>\n    <li><strong>Regularization</strong>: L1 or L2 regularization can also be added to the dense layer to prevent overfitting.</li>\n  </ul>\n  \n  <h4>Evaluation Metrics</h4>\n  \n  <p>Document the F1-Score, Precision, Recall, and AUC-ROC for each experiment.</p>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Initialize an empty list to store results\nresults_list = []\n\n# Define a function to build, train, and evaluate a model\ndef run_experiment(model, model_name, X_train, y_train, X_val, y_val, results_list):\n    # Compile and train the model\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    model.fit(X_train, y_train, epochs=CONFIG.epochs, batch_size=CONFIG.batch_size, validation_data=(X_val, y_val))\n    \n    # Evaluate the model\n    y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n    f1 = f1_score(y_val, y_pred)\n    precision = precision_score(y_val, y_pred)\n    recall = recall_score(y_val, y_pred)\n    auc_roc = roc_auc_score(y_val, y_pred)\n    \n    # Add the results to the list\n    results_list.append({'Experiment': model_name, 'F1-Score': f1, 'Precision': precision, 'Recall': recall, 'AUC-ROC': auc_roc})\n\n# Hyperparameter tuning for Base LSTM Model\ndef build_model(hp):\n    model = Sequential()\n    model.add(Embedding(CONFIG.max_vocab_size, hp.Int('embedding_dim', min_value=50, max_value=200, step=50), input_length=CONFIG.max_sequence_length))\n    model.add(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32)))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ntuner = RandomSearch(\n    build_model,\n    objective='val_accuracy',\n    max_trials=5,\n    directory='output',\n    project_name='NLP_Disaster_Tweets_Base_LSTM',\n    seed=42\n)\n\n\n# Start Keras Tuning\nprint(\"==========================================\")\nprint(\"========= Starting Keras Tuning ==========\")\nprint(\"==========================================\")\ntuner.search(X_train, y_train, epochs=CONFIG.epochs, validation_data=(X_val, y_val), verbose=1)\n\n# Get the optimal hyperparameters\nbest_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\nembedding_dim_best = best_hps.get('embedding_dim')\nlstm_units_best = best_hps.get('lstm_units')\n\n# Base LSTM Model with best hyperparameters\nprint(\"\\n\\n===========================================\")\nprint(\"====  Training Base LSTM Model (Tuned) ====\")\nprint(\"===========================================\")\nmodel1 = Sequential([\n    Embedding(CONFIG.max_vocab_size, embedding_dim_best, input_length=CONFIG.max_sequence_length),\n    LSTM(lstm_units_best),\n    Dense(1, activation='sigmoid')\n])\nrun_experiment(model1, 'Base LSTM Model (Tuned)', X_train, y_train, X_val, y_val, results_list)\n\n# LSTM with 100 Units\nprint(\"\\n\\n==========================================\")\nprint(\"=====  Training LSTM with 100 Units  =====\")\nprint(\"==========================================\")\nmodel2 = Sequential([\n    Embedding(CONFIG.max_vocab_size, embedding_dim_best, input_length=CONFIG.max_sequence_length),\n    LSTM(100),\n    Dense(1, activation='sigmoid')\n])\nrun_experiment(model2, 'LSTM with 100 Units', X_train, y_train, X_val, y_val, results_list)\n\n# GRU Model\nprint(\"\\n\\n==========================================\")\nprint(\"=========   Training GRU Model   =========\")\nprint(\"==========================================\")\nmodel3 = Sequential([\n    Embedding(CONFIG.max_vocab_size, embedding_dim_best, input_length=CONFIG.max_sequence_length),\n    GRU(lstm_units_best),\n    Dense(1, activation='sigmoid')\n])\nrun_experiment(model3, 'GRU Model', X_train, y_train, X_val, y_val, results_list)\n\n# Bidirectional LSTM\nprint(\"\\n\\n==========================================\")\nprint(\"====== Training Bidirectional LSTM  ======\")\nprint(\"==========================================\")\nmodel4 = Sequential([\n    Embedding(CONFIG.max_vocab_size, embedding_dim_best, input_length=CONFIG.max_sequence_length),\n    Bidirectional(LSTM(lstm_units_best)),\n    Dense(1, activation='sigmoid')\n])\nrun_experiment(model4, 'Bidirectional LSTM', X_train, y_train, X_val, y_val, results_list)\n\n# Stacked LSTM\nprint(\"\\n\\n==========================================\")\nprint(\"========= Training Stacked LSTM] =========\")\nprint(\"==========================================\")\nmodel5 = Sequential([\n    Embedding(CONFIG.max_vocab_size, embedding_dim_best, input_length=CONFIG.max_sequence_length),\n    LSTM(lstm_units_best, return_sequences=True),\n    LSTM(lstm_units_best),\n    Dense(1, activation='sigmoid')\n])\nrun_experiment(model5, 'Stacked LSTM', X_train, y_train, X_val, y_val, results_list)\n\n# LSTM with Batch Normalization\nprint(\"\\n\\n================================================\")\nprint(\"==== Training LSTM with Batch Normalization ====\")\nprint(\"================================================\")\nmodel6 = Sequential([\n    Embedding(CONFIG.max_vocab_size, embedding_dim_best, input_length=CONFIG.max_sequence_length),\n    LSTM(lstm_units_best),\n    BatchNormalization(),\n    Dense(1, activation='sigmoid')\n])\n\nrun_experiment(model6, 'LSTM with Batch Normalization', X_train, y_train, X_val, y_val, results_list)\n\n# Print Centered\ndef print_centered(df_str, title, width=80, char='='):\n    # Calculate the padding needed for each title\n    padding = (width - len(title)) // 2\n    \n    # Print the titles and DataFrame, centered\n    print(char * width)\n    print(\" \" * padding + title + \" \" * padding)\n    print(char * width)\n    print(df_str)\n    print(char * width)\n\n# Create a DataFrame from the results list\nresults_df = pd.DataFrame(results_list)\n\n# Display the results\nresults_str = results_df.to_string(index=False)\nprint_centered(results_str, \"Results\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T11:04:03.320600Z","iopub.execute_input":"2023-10-25T11:04:03.321043Z","iopub.status.idle":"2023-10-25T11:25:25.893612Z","shell.execute_reply.started":"2023-10-25T11:04:03.321001Z","shell.execute_reply":"2023-10-25T11:25:25.892451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: lightgrey; color: black; padding: 20px; border: 1px solid black;\">\n<h4>Results and Analysis</h4>\n<table border=\"1\">\n  <thead>\n    <tr>\n      <th>Experiment</th>\n      <th>F1-Score</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>AUC-ROC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>Base LSTM Model (Tuned)</td>\n      <td>0.708938</td>\n      <td>0.703030</td>\n      <td>0.714946</td>\n      <td>0.745345</td>\n    </tr>\n    <tr>\n      <td>LSTM with 100 Units</td>\n      <td>0.712538</td>\n      <td>0.707132</td>\n      <td>0.718028</td>\n      <td>0.748602</td>\n    </tr>\n    <tr>\n      <td>GRU Model</td>\n      <td>0.715180</td>\n      <td>0.726550</td>\n      <td>0.704160</td>\n      <td>0.753682</td>\n    </tr>\n    <tr>\n      <td>Bidirectional LSTM</td>\n      <td>0.706635</td>\n      <td>0.734219</td>\n      <td>0.681048</td>\n      <td>0.748991</td>\n    </tr>\n    <tr>\n      <td>Stacked LSTM</td>\n      <td>0.727273</td>\n      <td>0.704185</td>\n      <td>0.751926</td>\n      <td>0.758686</td>\n    </tr>\n    <tr>\n      <td>LSTM with Batch Normalization</td>\n      <td>0.717760</td>\n      <td>0.687853</td>\n      <td>0.750385</td>\n      <td>0.748762</td>\n    </tr>\n  </tbody>\n</table>\n\n  \n  <h5>Summary</h5>\n  \n  <ol>\n    <li><strong>F1-Score</strong>: The Stacked LSTM model has the highest F1-Score of approximately 0.727, indicating that this model performs the best in terms of both false positives and false negatives.</li>\n    <li><strong>Precision</strong>: The Bidirectional LSTM model has the highest precision of about 0.734, making it the most reliable when it predicts a positive class. However, high precision often comes at the expense of recall.</li>\n    <li><strong>Recall</strong>: The Stacked LSTM model also has the highest recall of approximately 0.752, indicating that this model is the most capable of identifying all possible positive samples.</li>\n    <li><strong>AUC-ROC</strong>: The Stacked LSTM model has the highest AUC-ROC score of about 0.759, which suggests that this model has the best ability to distinguish between the positive and negative classes.</li>\n    <li><strong>Overall Performance</strong>: The Stacked LSTM model outperforms in all metrics, making it a strong candidate for scenarios where both precision and recall are important.</li>\n  </ol>\n  \n  <p>It's important to consider the trade-offs between these metrics depending on the specific needs of your application. For example, if false positives are more costly, you might prioritize a model with higher precision. On the other hand, if missing a positive sample is more detrimental, a model with higher recall would be more appropriate.</p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"background-color: lightgray; color: black; border: 1px solid black; padding: 20px; margin: 20px;\">\n    <h3>Discussion and Interpretation of Results</h3>\n    <ol>\n        <li><strong>Model Comparison</strong>: The Stacked LSTM model showed the best performance in terms of F1-Score and AUC-ROC. This indicates that adding complexity in terms of stacking layers can indeed be beneficial for this specific problem.</li>\n        <li><strong>Trade-offs</strong>: Precision and Recall were often inversely related across the models. For instance, the Bidirectional LSTM model had the highest Precision but a lower Recall. This accentuates the importance of understanding the specific needs of the application to choose the right evaluation metric.</li>\n        <li><strong>Hyperparameter Tuning</strong>: The Base LSTM Model (Tuned) had a balanced performance across metrics, including the second-highest AUC-ROC score, showing the effectiveness of hyperparameter tuning.</li>\n    </ol>\n    <h3>Learnings and Takeaways</h3>\n    <ol>\n        <li><strong>Complexity vs. Performance</strong>: Contrary to initial observations, adding more complexity to the model through Stacked LSTM layers improved performance, particularly in terms of F1-Score and AUC-ROC.</li>\n        <li><strong>Importance of Normalization</strong>: Despite its lower precision, the LSTM model with Batch Normalization had one of the highest recalls and F1-Scores, reiterating the importance of feature scaling even in deep learning models.</li>\n        <li><strong>Metric Sensitivity</strong>: Different models excelled in different metrics, emphasizing the need to consider multiple evaluation metrics when assessing models.</li>\n    </ol>\n    <h3>What Worked and What Didn't</h3>\n    <ul>\n        <li><strong>Worked</strong>: Hyperparameter tuning, Stacked LSTM architecture, and GRU architecture.</li>\n        <li><strong>Didn't Work</strong>: Bidirectional LSTM did not significantly improve the metrics, despite its higher complexity.</li>\n    </ul>\n    <h3>Future Improvements</h3>\n    <ol>\n        <li><strong>Ensemble Methods</strong>: Combining the predictions from models like Stacked LSTM and GRU could potentially lead to more robust results.</li>\n        <li><strong>Data Augmentation</strong>: Techniques such as SMOTE could be employed to balance the class distribution, which might improve model performance.</li>\n        <li><strong>Feature Engineering</strong>: Additional text features like sentiment or text length could provide the model with more useful information.</li>\n        <li><strong>Advanced Architectures</strong>: Experimenting with newer architectures like Transformers could potentially yield even better results.</li>\n        <li><strong>Fine-Tuning</strong>: Using pre-trained embeddings for word representation could be another avenue for further improvements.</li>\n    </ol>\n    <p>In summary, the choice of model and evaluation metrics should align closely with the specific objectives of the application. Hyperparameter tuning remains a crucial step in model development, and it's essential to consider the trade-offs between different evaluation metrics.</p>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"## File Submission","metadata":{}},{"cell_type":"code","source":"# Importing necessary modules\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pandas as pd\n\n# Extracting features and labels from the training data\nX = train_data['text']\ny = train_data['target']\n\n# Configuration parameters\nCONFIG = {\n    'max_vocab_size': 5000,  # Maximum size of vocabulary\n    'max_sequence_length': 100,  # Maximum length of sequence\n    'embedding_dim': 100,  # Dimension of embedding\n    'lstm_units': 64,  # LSTM units\n    'batch_size': 32,  # Batch size\n    'epochs': 100  # Number of epochs\n}\n\n# Initialize the tokenizer\ntokenizer = Tokenizer(num_words=CONFIG['max_vocab_size'], oov_token='<OOV>')\ntokenizer.fit_on_texts(X)\n\n# Tokenize and pad sequences\nX_sequences = tokenizer.texts_to_sequences(X)\nX_padded = pad_sequences(X_sequences, maxlen=CONFIG['max_sequence_length'], padding='post', truncating='post')\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_padded, y, test_size=0.2, random_state=42)\n\n# Improved Stacked LSTM Model\nmodel = Sequential([\n    Embedding(CONFIG['max_vocab_size'], CONFIG['embedding_dim'], input_length=CONFIG['max_sequence_length']),\n    LSTM(CONFIG['lstm_units'], return_sequences=True),\n    Dropout(0.2),  # Added dropout layer\n    LSTM(CONFIG['lstm_units']),\n    BatchNormalization(),  # Added BatchNormalization layer\n    Dense(1, activation='sigmoid')\n])\n\n# Compile the model\noptimizer = Adam(learning_rate=0.001)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train, y_train, epochs=CONFIG['epochs'], batch_size=CONFIG['batch_size'], validation_data=(X_val, y_val))\n\n# Evaluate the model on the validation data\ny_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\nf1 = f1_score(y_val, y_pred)\nprecision = precision_score(y_val, y_pred)\nrecall = recall_score(y_val, y_pred)\nauc_roc = roc_auc_score(y_val, y_pred)\n\n# Output the evaluation metrics\nprint(f\"F1-Score: {f1}, Precision: {precision}, Recall: {recall}, AUC-ROC: {auc_roc}\")\n\n# Prepare the test data\nX_test_sequences = tokenizer.texts_to_sequences(test_data['text'])\nX_test_padded = pad_sequences(X_test_sequences, maxlen=CONFIG['max_sequence_length'], padding='post', truncating='post')\n\n# Make predictions on the test data\ntest_predictions = (model.predict(X_test_padded) > 0.5).astype(\"int32\")\n\n# Prepare the submission DataFrame\nsubmission_df = pd.DataFrame({'id': test_data['id'], 'target': test_predictions.flatten()})\n\n# Save the submission file\nsubmission_df.to_csv(\"submission.csv\", index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-25T12:04:27.086675Z","iopub.execute_input":"2023-10-25T12:04:27.087127Z","iopub.status.idle":"2023-10-25T12:08:30.368524Z","shell.execute_reply.started":"2023-10-25T12:04:27.087050Z","shell.execute_reply":"2023-10-25T12:08:30.367401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}